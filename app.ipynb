{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IY0OpaX8kbEHPsEUtPllyPxxN5mufxmx",
      "authorship_tag": "ABX9TyNjGZ5j3CIwaeuMidKicSg8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "wBfXGkvUXKhF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g-t4ozpiSMGw"
      },
      "outputs": [],
      "source": [
        "folder_path = \"/content/drive/MyDrive/Colab Notebooks/AB/spec/M5/project/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Fogbb_lTiIP",
        "outputId": "83e62433-3333-4928-c655-fcbfc5fa3f71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from PIL import Image\n",
        "from keras.models import load_model\n",
        "\n",
        "# Load the pretrained model\n",
        "pretrained_model = load_model(folder_path+'deep_fer_model_2.keras')\n",
        "pretrained_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# emotion labels\n",
        "emotion_labels = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
        "\n",
        "def preprocess_image(image):\n",
        "    \"\"\"\n",
        "    Preprocess the uploaded image to make it compatible with the model.\n",
        "    \"\"\"\n",
        "    img = image.convert('L')\n",
        "    img = img.resize((48, 48))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    reshaped = np.reshape(img_array, (1, 48, 48, 1))\n",
        "    return reshaped\n",
        "\n",
        "def predict_emotion(image):\n",
        "    \"\"\"\n",
        "    Predict the emotion of the face in the uploaded image.\n",
        "    \"\"\"\n",
        "    processed_image = preprocess_image(image)\n",
        "    predictions = pretrained_model.predict(processed_image)\n",
        "    label = np.argmax(predictions)\n",
        "    confidence = np.max(predictions)\n",
        "\n",
        "    # messages based on emotion\n",
        "    if label == 0:  # Angry\n",
        "        message = \"You look angry! ğŸ˜¡ Maybe take a moment to cool down?\"\n",
        "    elif label == 1:  # Disgust\n",
        "        message = \"You seem disgusted. ğŸ˜– What's bothering you?\"\n",
        "    elif label == 2:  # Fear\n",
        "        message = \"You appear fearful. ğŸ˜¨ Don't worry, you're safe here!\"\n",
        "    elif label == 3:  # Happy\n",
        "        message = \"It looks like you're smiling! ğŸ˜Š Keep spreading positivity!\"\n",
        "    elif label == 4:  # Neutral\n",
        "        message = \"You seem calm and neutral. ğŸ˜Œ A moment of peace is always good.\"\n",
        "    elif label == 5:  # Sad\n",
        "        message = \"It seems you're feeling down. ğŸ˜” Remember, it's okay to feel sad sometimes.\"\n",
        "    elif label == 6:  # Surprise\n",
        "        message = \"You look surprised! ğŸ˜² Did something unexpected happen?\"\n",
        "    else:\n",
        "        message = \"Interesting! Your facial expression suggests a unique emotion.\"\n",
        "\n",
        "    return emotion_labels[label], f\"{confidence:.2%}\", message\n",
        "\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict_emotion,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload an Image\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Predicted Emotion\"),\n",
        "        gr.Textbox(label=\"Confidence\"),\n",
        "        gr.Textbox(label=\"Message\")\n",
        "    ],\n",
        "    title=\"Facial Emotion Recognition\",\n",
        "    description=\"Upload a photo to identify the emotion displayed using a pre-trained CNN.\"\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "zTHYNT9RTkkh",
        "outputId": "7932c075-e2a2-4767-8f6f-da06d1e439a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ac82c91a63e35179ac.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ac82c91a63e35179ac.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378ms/step\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://ac82c91a63e35179ac.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dfVkYhDkeydH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion for the Facial Emotion Recognition App\n",
        "\n",
        "The **Facial Emotion Recognition App** demonstrates the power of deep learning in analyzing and interpreting human emotions from facial expressions. By leveraging a pre-trained Convolutional Neural Network (CNN), the app provides an interactive, user-friendly platform for users to explore the capabilities of artificial intelligence in understanding emotions. Below is a detailed summary of its functionality, significance, and future potential:\n",
        "\n",
        "\n",
        "### **Functionality**\n",
        "1. **Emotion Detection**  \n",
        "   The app identifies seven key emotionsâ€”Angry, Disgust, Fear, Happy, Neutral, Sad, and Surpriseâ€”based on facial expressions in uploaded images. It provides real-time feedback on the dominant emotion along with the confidence score.\n",
        "\n",
        "2. **User Interaction**  \n",
        "   Users can easily upload images of faces, and the app preprocesses these images (grayscale conversion, resizing, normalization) to ensure compatibility with the CNN model.\n",
        "\n",
        "3. **Feedback Messages**  \n",
        "   For each predicted emotion, the app offers a thoughtful message to engage users and provide a personalized experience. These messages are crafted to add a human touch, making the app not only a technological tool but also an empathetic one.\n",
        "\n",
        "4. **Seamless Interface**  \n",
        "   Using Gradio, the app presents a clean and intuitive interface that allows users to interact effortlessly with the AI model. The interface includes upload functionality, clear predictions, and confidence levels.\n",
        "\n",
        "\n",
        "### **Significance**\n",
        "1. **Human-AI Interaction**  \n",
        "   The app bridges the gap between humans and AI by enabling machines to interpret human emotionsâ€”a significant milestone in the field of artificial intelligence.\n",
        "\n",
        "2. **Applications**  \n",
        "   - **Healthcare:** The app could assist in diagnosing mental health conditions by analyzing changes in emotional expressions.  \n",
        "   - **Education:** Teachers and educational platforms could use such technology to gauge student engagement and emotional states.  \n",
        "   - **Customer Service:** Businesses could deploy similar solutions to understand customer emotions during interactions.  \n",
        "\n",
        "3. **Demonstration of Deep Learning**  \n",
        "   This project showcases how CNNs can handle complex, unstructured data like images to produce meaningful insights, making it an excellent educational tool for those interested in deep learning.\n",
        "\n",
        "\n",
        "The Facial Emotion Recognition App is a powerful illustration of how deep learning can interpret one of the most complex and human aspects of communicationâ€”emotions. This project is not only a technical accomplishment but also a stepping stone toward creating more empathetic and responsive AI systems. By building upon this foundation, future developments can make AI even more impactful in enhancing human lives."
      ],
      "metadata": {
        "id": "_pAblfP5VYil"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z21AUHObUIcI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}